{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import TypedDict, List, Dict, Optional\n",
    "import os\n",
    "import json\n",
    "import dotenv\n",
    "\n",
    "# LLM and embedding related imports\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Evaluation related imports\n",
    "from ragas.metrics import AnswerAccuracy, ContextRelevance, ResponseGroundedness\n",
    "from ragas.dataset_schema import SingleTurnSample, EvaluationDataset\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "# Load environment variables\n",
    "dotenv.load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and Vector Store Setup\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "vectorstore = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Loading\n",
    "\n",
    "docs = []\n",
    "data_folder = \"data\"\n",
    "\n",
    "if os.path.exists(data_folder) and os.path.isdir(data_folder):\n",
    "    for filename in os.listdir(data_folder):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(data_folder, filename)\n",
    "            try:\n",
    "                loader = PyPDFLoader(file_path)\n",
    "                file_docs = loader.load()\n",
    "                docs.extend(file_docs)\n",
    "                print(f\"Loaded {len(file_docs)} documents from {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filename}: {e}\")\n",
    "else:\n",
    "    print(f\"Folder {data_folder} does not exist or is not a directory\")\n",
    "\n",
    "print(f\"Total documents loaded: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Document Processing and Vectorization\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "print(\"Splitting documents into chunks...\")\n",
    "splitted_docs = text_splitter.split_documents(docs)\n",
    "print(f\"Splitted documents into: {len(splitted_docs)} chunks\")\n",
    "\n",
    "print(\"Adding documents to vector store...\")\n",
    "_ = vectorstore.add_documents(splitted_docs)\n",
    "print(\"Documents added to vector store\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Pipeline Implementation with LangGraph\n",
    "\n",
    "class State(TypedDict):\n",
    "    query: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "    \n",
    "def retrieve(state: State):\n",
    "    print(f\"üîç Retrieving documents for: {state['query']}\")\n",
    "    retrieved_informations = vectorstore.similarity_search(query=state[\"query\"], k=2)\n",
    "    return {\"context\": retrieved_informations}\n",
    "\n",
    "def generate_answer(state: State):\n",
    "    print(\"üí¨ Generating answer...\")\n",
    "    \n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful AI assistant. Answer the question based only on the context provided. \n",
    "    If the context doesn't contain the information needed to answer the question, say \"I don't have enough information to answer this question.\"\n",
    "    \n",
    "    Question: {state[\"query\"]}\n",
    "    Context: {docs_content}\n",
    "    \n",
    "    Provide a concise and accurate answer based solely on the information in the context:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# Build RAG pipeline graph\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"retrieve\", retrieve)\n",
    "graph_builder.add_node(\"generate_answer\", generate_answer)\n",
    "\n",
    "# Connect nodes in sequence\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph_builder.add_edge(\"retrieve\", \"generate_answer\")\n",
    "graph_builder.add_edge(\"generate_answer\", END)\n",
    "\n",
    "# Compile the graph\n",
    "graph = graph_builder.compile()\n",
    "print(\"‚úì RAG pipeline ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke({\"query\": \"what is deep mind?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Load and Explore Test Dataset\n",
    "print(\"Loading test dataset...\")\n",
    "test_data = pd.read_csv(\"test-set/synthetic-test-set.csv\")\n",
    "print(f\"Loaded {len(test_data)} test examples with query - ground_truth pairs\")\n",
    "test_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing of Test Questions\n",
    "\n",
    "print(f\"Processing {len(test_data)} test questions...\")\n",
    "results = []\n",
    "\n",
    "for index, row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Generating answers\"):\n",
    "    query = row['query']\n",
    "    ground_truth = row['ground_truth']\n",
    "    \n",
    "    result = graph.invoke({\"query\": query})\n",
    "    \n",
    "    results.append({\n",
    "        \"query\": query,\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"response\": result[\"answer\"],\n",
    "        \"contexts\": [doc.page_content for doc in result[\"context\"]]\n",
    "    }) \n",
    "    \n",
    "   \n",
    "\n",
    "# Save final results to JSON file\n",
    "with open(\"test_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Successfully generated answers for {len(results)} questions\")\n",
    "print(f\"Results saved to 'test_results.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Evaluation Dataset for RAGAS\n",
    "print(\"Preparing evaluation dataset...\")\n",
    "samples = []\n",
    "\n",
    "# Convert results to RAGAS format\n",
    "for item in tqdm(results, desc=\"Converting to RAGAS format\"):\n",
    "    sample = SingleTurnSample(\n",
    "        user_input=item[\"query\"],\n",
    "        response=item[\"response\"],\n",
    "        reference=item[\"ground_truth\"],\n",
    "        retrieved_contexts=item[\"contexts\"]\n",
    "    )\n",
    "    samples.append(sample)\n",
    "\n",
    "# Create evaluation dataset\n",
    "dataset = EvaluationDataset(samples=samples)\n",
    "print(f\"‚úì Prepared evaluation dataset with {len(samples)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAGAS Evaluation with Visualizations\n",
    "\n",
    "# Initialize evaluation metrics\n",
    "metrics = [\n",
    "    AnswerAccuracy(),   # Measures correctness against reference answer\n",
    "    ContextRelevance(), # Measures relevance of retrieved context\n",
    "    ResponseGroundedness() # Measures if response is grounded in context\n",
    "]\n",
    "\n",
    "print(\"Running RAGAS evaluation...\")\n",
    "# Wrap LLM for RAGAS\n",
    "ragas_llm = LangchainLLMWrapper(llm)\n",
    "# Run evaluation\n",
    "result = evaluate(\n",
    "    dataset=dataset,\n",
    "    metrics=metrics,\n",
    "    llm=ragas_llm\n",
    ")\n",
    "print(\"‚úì Evaluation complete\")\n",
    "print(result)\n",
    "result.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
